import streamlit as st
import uuid
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_ollama import ChatOllama
from dotenv import load_dotenv
from retriever import FanalcaRetriever
from structured_tool import FanalcaStructuredTool

# ==========================================================
# âš™ï¸ CONFIGURACIÃ“N INICIAL
# ==========================================================
st.set_page_config(page_title="Fanalca Bot", page_icon="ğŸ¤–", layout="centered")
load_dotenv()

st.markdown("""
# ğŸ¤– Fanalca Bot  
_Asistente virtual corporativo de Fanalca S.A._

ğŸ’¡ Puedes preguntar sobre historia, sostenibilidad, negocios o datos de contacto (NIT, correo, telÃ©fono, etc.).
""")

# ==========================================================
# ğŸšï¸ CONTROL DE TEMPERATURA
# ==========================================================
temperature = st.sidebar.slider(
    "Creatividad del modelo (temperature)",
    0.0, 1.5, 0.7, 0.1,
    help="Valores bajos â†’ respuestas mÃ¡s precisas. Valores altos â†’ mÃ¡s creativas."
)

# ==========================================================
# ğŸ§© DEFINICIÃ“N DEL ESTADO
# ==========================================================
class State(TypedDict):
    messages: Annotated[list, add_messages]

# ==========================================================
# ğŸ¤– CONFIGURACIÃ“N DEL MODELO Y HERRAMIENTAS
# ==========================================================
llm = ChatOllama(model="gemma3:1b", temperature=temperature)
retriever = FanalcaRetriever("fanalca_knowledge_base_final.json")
structured_tool = FanalcaStructuredTool("structured_data.json")

# ==========================================================
# ğŸ§  META-PROMPT DEL AGENTE ROUTER
# ==========================================================
ROUTER_PROMPT = """
Eres el Agente Enrutador Inteligente de FANALCA BOT.
Debes decidir cuÃ¡l herramienta responderÃ¡ la consulta del usuario:

1ï¸âƒ£ Structured Tool (JSON estructurado) â†’ Datos concretos: correo, telÃ©fono, NIT, direcciÃ³n, sedes, redes, horarios.
2ï¸âƒ£ RAG Retriever (base vectorial) â†’ InformaciÃ³n general: historia, proyectos, sostenibilidad, misiÃ³n, visiÃ³n, valores.

Responde SOLO con una palabra:
STRUCTURED o RAG
"""

# ==========================================================
# ğŸš¦ FUNCIÃ“N DE ENRUTAMIENTO
# ==========================================================
def route_query(user_query: str) -> str:
    q = user_query.lower().strip()

    # HeurÃ­stica rÃ¡pida
    structured_keywords = [
        "correo", "email", "telÃ©fono", "telefono", "direcciÃ³n", "ubicaciÃ³n",
        "nit", "sede", "horario", "redes", "instagram", "linkedin",
        "facebook", "servicio", "atenciÃ³n"
    ]
    if any(k in q for k in structured_keywords):
        st.session_state["last_route"] = "STRUCTURED"
        return "STRUCTURED"

    # Si no hay coincidencia, usa el modelo para decidir
    try:
        decision = llm.invoke([
            {"role": "system", "content": ROUTER_PROMPT},
            {"role": "user", "content": user_query}
        ])
        route = decision.content.strip().upper()
        if route not in ["STRUCTURED", "RAG"]:
            route = "RAG"
        st.session_state["last_route"] = route
        return route
    except Exception as e:
        print("âš ï¸ Error en router:", e)
        st.session_state["last_route"] = "RAG"
        return "RAG"

# ==========================================================
# ğŸ’¬ FUNCIÃ“N PRINCIPAL DEL CHATBOT
# ==========================================================
def chatbot(state: State):
    last_user_msg = ""
    for m in reversed(state["messages"]):
        if isinstance(m, tuple) and m[0] == "user":
            last_user_msg = m[1]
            break
        if isinstance(m, dict) and m.get("role") == "user":
            last_user_msg = m.get("content", "")
            break

    print(f"\nğŸ—£ï¸ Usuario: {last_user_msg}")

    # Determinar ruta
    route = route_query(last_user_msg)
    print(f"ğŸš¦ Ruta elegida: {route}")

    # STRUCTURED â†’ usar JSON
    if route == "STRUCTURED":
        structured_response = structured_tool.get_info(last_user_msg).strip()
        print("âœ… Structured Tool â†’", structured_response)

        if structured_response and "No tengo informaciÃ³n" not in structured_response:
            return {"messages": [{"role": "assistant", "content": structured_response}]}
        else:
            print("âš ï¸ Structured vacÃ­o, pasando a RAG...")
            route = "RAG"

    # RAG â†’ usar base vectorial
    print("ğŸ“˜ Usando RAG Retriever")
    context = retriever.build_context(last_user_msg, top_k=4)
    system_prompt = f"""
Eres un asistente virtual experto y confiable especializado exclusivamente en la empresa fanalca

Tu objetivo es responder con precisiÃ³n, claridad y lenguaje formal, utilizando Ãºnicamente la informaciÃ³n contenida en el contexto siguiente, que proviene del sitio web oficial de Fanalca y sus fuentes verificadas:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{context}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ’¬ Instrucciones importantes:

1. Analiza el contexto con atenciÃ³n. Si el usuario pregunta por elementos como **misiÃ³n**, **visiÃ³n**, **valores**, **propÃ³sito**, **pilares estratÃ©gicos**, **historia**, **unidades de negocio**, **sostenibilidad** o **FundaciÃ³n Fanalca**, busca tÃ©rminos relacionados en el contexto aunque no estÃ©n escritos exactamente igual.
   - Por ejemplo, si el contexto menciona â€œpropÃ³sito superiorâ€ en lugar de â€œmisiÃ³nâ€, explica que ese es el equivalente a la misiÃ³n corporativa.
   - Si el texto habla de â€œvisiÃ³n de construcciÃ³n colectivaâ€, puedes interpretarlo como la visiÃ³n institucional.

2. Si la informaciÃ³n no aparece en el contexto o no tiene relaciÃ³n con fanalca, responde amablemente:
   ğŸ‘‰ â€œLo siento, no tengo esa informaciÃ³n disponible en este momento porque mi conocimiento se limita a fanalcaâ€

3. No inventes informaciÃ³n externa, recetas, chistes o temas que no estÃ©n vinculados a fanalca.

4. Responde de manera profesional, clara y con redacciÃ³n natural, como si fueras un asistente corporativo de fanalca.

5. Si no encuentras la informaciÃ³n en el context, responde con:
    "Lo siento, no tengo esa informaciÃ³n disponible en este momento."
    """

    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}

# ==========================================================
# ğŸ”— GRAFO CONVERSACIONAL
# ==========================================================
graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)
memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory)

# ==========================================================
# ğŸ’¬ INTERFAZ STREAMLIT
# ==========================================================
if "history" not in st.session_state:
    st.session_state["history"] = []
if "thread_id" not in st.session_state:
    st.session_state["thread_id"] = f"user-{uuid.uuid4().hex[:8]}"
if "last_route" not in st.session_state:
    st.session_state["last_route"] = "â€”"

if st.sidebar.button("ğŸ§¹ Nueva conversaciÃ³n"):
    st.session_state["history"] = []
    st.session_state["thread_id"] = f"user-{uuid.uuid4().hex[:8]}"
    st.rerun()

def chat_with_memory(user_input):
    # Si la consulta es estructurada, responder directamente SIN pasar por el grafo
    route = route_query(user_input)
    if route == "STRUCTURED":
        structured_response = structured_tool.get_info(user_input).strip()
        if structured_response and "No tengo informaciÃ³n" not in structured_response:
            st.session_state["last_route"] = "STRUCTURED"
            return structured_response
        else:
            route = "RAG"  # fallback si no hay match

    # Si no fue structured o no hubo dato â†’ usar RAG dentro del grafo
    config = {"configurable": {"thread_id": st.session_state["thread_id"]}}
    result = graph.invoke({"messages": [("user", user_input)]}, config=config)
    last_msg = result["messages"][-1]

    st.session_state["last_route"] = "RAG"

    if isinstance(last_msg, dict) and "content" in last_msg:
        return last_msg["content"]
    if hasattr(last_msg, "content"):
        return last_msg.content
    return str(last_msg)


st.markdown("---")
st.subheader("ğŸ’¬ Chat con Fanalca Bot")

user_input = st.chat_input("Escribe tu pregunta aquÃ­...")

if user_input:
    with st.spinner("Pensando..."):
        response = chat_with_memory(user_input)
        route = st.session_state.get("last_route", "RAG")
        st.session_state["history"].append({"user": user_input, "bot": response, "route": route})

for chat in st.session_state["history"]:
    with st.chat_message("user"):
        st.markdown(chat["user"])
    with st.chat_message("assistant"):
        st.markdown(f"**[{chat['route']}]** {chat['bot']}")

st.sidebar.markdown("### ğŸ§­ Ãšltima ruta usada:")
st.sidebar.write(f"**{st.session_state['last_route']}**")

with st.sidebar.expander("ğŸ“œ Historial de conversaciÃ³n"):
    for i, chat in enumerate(st.session_state["history"], 1):
        st.markdown(f"**{i}. Usuario:** {chat['user']}")
        st.markdown(f"**ğŸ¤– ({chat['route']})** {chat['bot']}")
        st.markdown("---")
